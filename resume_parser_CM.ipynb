{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386d5a7d",
   "metadata": {},
   "source": [
    "# Extract information from Resume using pyresparser and other resume libraries\n",
    "## download spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e470a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (57.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/chris/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.60.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chris/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/chris/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/chris/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/chris/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "\u001b[33mWARNING: Error parsing requirements for keras: [Errno 2] No such file or directory: '/home/chris/anaconda3/lib/python3.8/site-packages/Keras-2.4.3.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for absl-py: [Errno 2] No such file or directory: '/home/chris/anaconda3/lib/python3.8/site-packages/absl_py-0.12.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "/home/chris/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package words to /home/chris/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# pip install pyresparser\n",
    "\n",
    "# spaCy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# nltk\n",
    "!python -m nltk.downloader words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f12b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/chris/anaconda3/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/home/chris/anaconda3/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "spacy.cli.download(\"en\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65eb5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the resume parser from pyresparser\n",
    "from pyresparser import ResumeParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd4f2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "data = ResumeParser('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4dc896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Sydney (UTS)', 'email': 'amengly@gmail.com', 'mobile_number': None, 'skills': ['System', 'Email', 'English', 'Data entry', 'Accounting', 'Mock', 'Mobile', 'Reports', 'Schedule', 'French', 'Communication', 'Photography', 'Green', 'Small business', 'Technical'], 'college_name': None, 'degree': None, 'designation': None, 'experience': ['continuous improving my understanding of accounting skill in my third year at the', 'University.', 'July 2019 – January 2021', 'March 2017 – June 2019', 'December 2015 – November 2016', 'March 2013 – July 2015', 'January 2009 – August 2014'], 'company_names': ['▪'], 'no_of_pages': 3, 'total_experience': 12.58}\n"
     ]
    }
   ],
   "source": [
    "print(data.get_extracted_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0700a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name  :  Sydney (UTS)\n",
      "email  :  amengly@gmail.com\n",
      "mobile_number  :  None\n",
      "skills  :  ['Email', 'Accounting', 'System', 'Mobile', 'Green', 'Mock', 'English', 'Communication', 'Small business', 'Schedule', 'Technical', 'Data entry', 'French', 'Photography', 'Reports']\n",
      "college_name  :  None\n",
      "degree  :  None\n",
      "designation  :  None\n",
      "experience  :  ['continuous improving my understanding of accounting skill in my third year at the', 'University.', 'July 2019 – January 2021', 'March 2017 – June 2019', 'December 2015 – November 2016', 'March 2013 – July 2015', 'January 2009 – August 2014']\n",
      "company_names  :  ['▪']\n",
      "no_of_pages  :  3\n",
      "total_experience  :  12.58\n"
     ]
    }
   ],
   "source": [
    "for key, value in data.get_extracted_data().items():\n",
    "    print(key, ' : ', value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6b8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "2021-07-19 11:25:12,548 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
      "2021-07-19 11:25:27,165 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
      "2021-07-19 11:25:29,552 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    }
   ],
   "source": [
    "from resume_parser import resumeparse\n",
    "data1 = resumeparse.read_file('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6de038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0350356f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using resume_parser\n",
    "\n",
    "#from resume_parser import resumeparse\n",
    "#data1 = resumeparse.read_file('/home/chris/reesby/reverse_Malih/Accountant_Sydney/AudreyPritchardResume-latest.docx') #Resume.pdf')\n",
    "\n",
    "#for key, value in data1.items():\n",
    "#    print(key, ' : ', value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eefc66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email  :  stormy_pete_1@yahoo.com\n",
      "phone  :  +61 404 123 333\n",
      "name  :  Personal Statement\n",
      "total_exp  :  0\n",
      "university  :  ['macquarie university', 'monash university', 'ie university']\n",
      "designition  :  ['operational meteorologist', 'java programmer', 'weather forecaster', 'early morning']\n",
      "degree  :  []\n",
      "skills  :  ['operating systems', 'windows', 'store', 'interview', 'completion', 'r', 'linux', 'tomcat', 'servlets', 'fog', 'player', 'decision-making', 'web', 'project', 'events', 'mvc', 'web pages', 'golf', 'aviation', 'drive', 'air', 'atmospheric science', 'controllers', 'learning', 'sql', 'architecture', 'it', 'web applications', 'airline', 'operations', 'video', 'data science', 'databases', 'clear', 'ux', 'rolling', 'computing', 'teams', 'use case', 'meteorology', 'email', 'machine learning', 'writers', 'maintenance', 'data manipulation', 'java', 'accountability', 'oracle database', 'visualisation', 'forecasting', 'case', 'apache', 'database', 'resume', 'performing', 'pressure', 'pages', 'airlines', 'python', 'history', 'football', 'training', 'weather', 'oracle']\n",
      "Companies worked at  :  ['Sun certification', 'Sun Certification']\n"
     ]
    }
   ],
   "source": [
    "from resume_parser import resumeparse\n",
    "data1 = resumeparse.read_file('/home/chris/reesby/reverse_Malih/new_resumes/Peter-Ziminovic-Resume.docx') #Resume.pdf')\n",
    "\n",
    "for key, value in data1.items():\n",
    "    print(key, ' : ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8a5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    #for line in text:\n",
    "    #    print(line)\n",
    "    print(text)\n",
    "    return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f619c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_text_from_pdf('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "792fb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be44c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(extract_text_from_docx('/home/chris/reesby/reverse_Malih/Accountant_Sydney/AudreyPritchardResume-latest.docx')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d64ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chris/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/chris/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/chris/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Using nltk to tokenize and exract names - doesn't really work\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "610de1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_docx('/home/chris/reesby/reverse_Malih/Accountant_Sydney/AudreyPritchardResume-latest.docx')\n",
    "names = extract_names(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b92c0c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Audrey', 'Pritchard Senior Financial Manager Sydney', 'Education', 'Public Health', 'Summary Collaborative', 'Career', 'Royal', 'Administered', 'Life Ltd', 'Enabled', 'Cancer Council', 'Administered', 'Controller', 'State Family', 'Purchase Order System', 'Authorisation Processes', 'Payroll Policies', 'Glen Street Theatre', 'Warringah Council', 'Marketing', 'Education Bachelor', 'Certifications Charted Institute', 'Sydney TAFE Corporations Law', 'Sydney TAFE Software Skills Oracle', 'Pastel', 'Payroll', 'Micros Fidelio', 'Microsoft Dynamics']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70979b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess  # noqa: S404\n",
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "\n",
    "def doc_to_text_catdoc(file_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(  # noqa: S607,S603\n",
    "            ['catdoc', '-w', file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "    except (\n",
    "        FileNotFoundError,\n",
    "        ValueError,\n",
    "        subprocess.TimeoutExpired,\n",
    "        subprocess.SubprocessError,\n",
    "    ) as err:\n",
    "        return (None, str(err))\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "    return (stdout.strip(), stderr.strip())\n",
    "\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93262de6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_phone_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_256640/583552459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphone_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_phone_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(phone_number)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_phone_number' is not defined"
     ]
    }
   ],
   "source": [
    "phone_number = extract_phone_number(text)\n",
    "#print(phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e771d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb6d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ea470ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "def convert_pdf_to_string(file_path):\n",
    "\n",
    "    output_string = StringIO()\n",
    "    with open(file_path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "    return(output_string) #.getvalue())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98829154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_pdf_to_string('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842c6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cb6b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/chris/anaconda3/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/home/chris/anaconda3/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd16b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# output_string = convert_pdf_to_string('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')\n",
    "# #print(output_string.getvalue()) \n",
    "# contents = output_string.getvalue()\n",
    "# #for line in contents : #output_string.getvalue():\n",
    "# #    print(line)\n",
    "# #print(contents)\n",
    "\n",
    "# #from nltk.tokenize import word_tokenize\n",
    "# #print(word_tokenize(contents))\n",
    "# #for word in word_tokenize(contents):\n",
    "# #    print(word)\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import spacy\n",
    "# #nlp =spacy.load('en_core_web_sm')\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "# sent_text = nltk.sent_tokenize(contents)\n",
    "# for sen in sent_text:\n",
    "#     print(sen)\n",
    "#     word_tokens = nltk.word_tokenize(sen)\n",
    "#     for word in word_tokens:\n",
    "#         tags = nlp(word)\n",
    "#         for ent in tags.ents:\n",
    "#             print(word,ent.text, ent.label_) # ent.start_char, ent.end_char,\n",
    "#     #for word in word_tokens:\n",
    "#     #    print(word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b916bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c359be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb51f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing required modules \n",
    "# import PyPDF2 \n",
    "    \n",
    "# # creating a pdf file object \n",
    "# pdfFileObj = open('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf', 'rb') \n",
    "    \n",
    "# # creating a pdf reader object \n",
    "# pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "    \n",
    "# # printing number of pages in pdf file \n",
    "# print(pdfReader.numPages) \n",
    "    \n",
    "# # creating a page object \n",
    "# pageObj = pdfReader.getPage(0) \n",
    "    \n",
    "# # extracting text from page \n",
    "# print(pageObj.extractText()) \n",
    "    \n",
    "# # closing the pdf file object \n",
    "# pdfFileObj.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef081e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3904d03e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E201] Span index out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-56d1b9583d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-56d1b9583d0b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResumeParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/chris/reesby/reverse_Malih/Accountant_Sydney/AudreyPritchardResume-latest.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m#rp = ResumeParser('./Sample Data/Deepak Kumar_Business Analyst_PM-NC.docx')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-56d1b9583d0b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResumeParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Converting Docx to txt using docx2txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-56d1b9583d0b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0msection_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrule_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m#print(rule_id, data[start:end].text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E201] Span index out of range."
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# PHONE_NO_PATTERN = re.compile(r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}[\\s]+)\")\n",
    "\n",
    "class MatchEvent:\n",
    "    PERSON_PATTERN      = [{'POS': 'PROPN', 'ENT_TYPE': 'PERSON'},\n",
    "                          {'POS': 'PROPN', 'ENT_TYPE': 'PERSON', 'OP': '?'},\n",
    "                          {'POS': 'PROPN', 'ENT_TYPE': 'PERSON'}]\n",
    "\n",
    "    EMAIL_ID_PATTERN    = [{\"LIKE_EMAIL\": True}]\n",
    "\n",
    "    PHONE_PATTERN_1     = [{\"SHAPE\": \"ddd\"}, {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "                           {\"SHAPE\": \"ddd\"}, {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "                           {\"SHAPE\": \"dddd\"}]\n",
    "\n",
    "    PHONE_PATTERN_2     = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"},\n",
    "                           {\"SHAPE\": \"ddd\"}, {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "                           {\"SHAPE\": \"dddd\", \"LENGTH\": {\"==\": 4}}]\n",
    "\n",
    "    ADDRESS_PATTERN     = [{'POS': 'NUM'}, {'POS': 'PROPN'},\n",
    "                           {'POS': 'PROPN', 'OP': \"*\"},\n",
    "                           {'IS_PUNCT': True, 'OP': '?'},\n",
    "                           {'LEMMA': {'IN': ['apt', 'unit']}, 'OP': '?'},\n",
    "                           {'POS': 'NUM'},\n",
    "                           {'ORTH': ',', 'OP': \"?\"},\n",
    "                           {'POS': 'PROPN'},\n",
    "                           {'ORTH': {\n",
    "                               'REGEX': '(AK|AL|AR|AZ|CA|CO|CT|DC|DE|FL|GA|GU|HI|IA|ID|IL|IN|KS|KY|LA|MA|MD|ME|MI|MN|MO|MS|MT|NC|ND|NE|NH|NJ|NM|NV|NY|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VA|VI|VT|WA|WI|WV|WY)'}}\n",
    "                           # ,{'IS_DIGIT': True, 'LENGTH': 5}\n",
    "                           ]\n",
    "\n",
    "    LINKEDIN_URL_PATTERN = [{\"LIKE_URL\": True, 'ORTH': {'REGEX': r'\\s*(linkedin.com)\\s*'}}]\n",
    "\n",
    "    GIT_URL_PATTERN     = [{\"LIKE_URL\": True, 'ORTH': {'REGEX': r'\\s*(github.com)\\s*'}}]\n",
    "\n",
    "    @classmethod\n",
    "    def full_name_event(cls, matcher, doc, i, matches):\n",
    "        match_id, start, end = matches[i]\n",
    "        entity = Span(doc, start, end, label=\"EVENT\")\n",
    "        if i == 2:\n",
    "            full_name = doc[matches[0][1]:matches[0][2]]\n",
    "            if str(entity.text).startswith(str(full_name)):\n",
    "                matches[0] = matches[i]\n",
    "\n",
    "    @classmethod\n",
    "    def summary_text_event(cls, matcher, doc, i, matches):\n",
    "        match_id, start, end = matches[i]\n",
    "        entity = Span(doc, start, end, label=\"EVENT\")\n",
    "\n",
    "\n",
    "\n",
    "class ResumeParser:\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    CANDIDATE_INFO = [{'id': 'FullName',    'match_on': MatchEvent.full_name_event, 'pattern': MatchEvent.PERSON_PATTERN},\n",
    "                      {'id': 'Email',       'match_on': None,                       'pattern': MatchEvent.EMAIL_ID_PATTERN},\n",
    "                      {'id': 'Address',     'match_on': None,                       'pattern': MatchEvent.ADDRESS_PATTERN},\n",
    "                      {'id': 'Phone',       'match_on': None,                       'pattern': MatchEvent.PHONE_PATTERN_1},\n",
    "                      {'id': 'Phone',       'match_on': None,                       'pattern': MatchEvent.PHONE_PATTERN_2},\n",
    "                      {'id': 'GithubURL',   'match_on': None,                       'pattern': MatchEvent.GIT_URL_PATTERN},\n",
    "                      {'id': 'LinkedInURL', 'match_on': None,                       'pattern': MatchEvent.LINKEDIN_URL_PATTERN}]\n",
    "\n",
    "    SECTION_TITLE = ['CandidateInformation', 'SummaryText', 'ToolsAndTechnologies', 'WorkExperience',\n",
    "                 'Education', 'Extra-curricular', 'AwardsAndRecognition']\n",
    "\n",
    "    SECTION_INFO_FILE = './section_title.csv'\n",
    "\n",
    "    PROFILE_INFORMATION = {}\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        self.txt = self.convert_docx2txt(file_name)\n",
    "        self.doc = ResumeParser.nlp(self.txt)\n",
    "        self.matcher = Matcher(self.nlp.vocab, validate=True)\n",
    "        self.section_data = self.load_data(self.doc)\n",
    "\n",
    "    # Converting Docx to txt using docx2txt\n",
    "    def convert_docx2txt(self, file_name):\n",
    "        temp = docx2txt.process(file_name)\n",
    "        #text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "        #return ' '.join(temp)\n",
    "        return temp\n",
    "\n",
    "    def load_data(self, data):\n",
    "        section_dict = pd.read_csv(self.SECTION_INFO_FILE)\n",
    "        section_words = {}\n",
    "        matcher = PhraseMatcher(self.nlp.vocab)\n",
    "        for section in self.SECTION_TITLE[1:]:\n",
    "            section_words[section] = [self.nlp(text) for text in section_dict[section].dropna(axis=0)]\n",
    "            matcher.add(section, None, *section_words[section])\n",
    "\n",
    "        section_data = {}\n",
    "        matches = matcher(data)\n",
    "\n",
    "        if len(matches) > 0:\n",
    "            section_data[self.SECTION_TITLE[0]] = str(data[:matches[0][1]-1])\n",
    "\n",
    "        for index, section in enumerate(matches):\n",
    "            match_id, start, end = section\n",
    "            rule_id = self.nlp.vocab.strings[match_id]\n",
    "\n",
    "            if index == len(matches) - 1:\n",
    "                span = data[end:]\n",
    "            else:\n",
    "                span = data[end: matches[index + 1][1] - 1]\n",
    "\n",
    "            if str(span.text) != '':\n",
    "                section_data[rule_id] = str(span.text)\n",
    "            #print(rule_id, data[start:end].text)\n",
    "            #print('{}    -    {}'.format(rule_id, span.string))\n",
    "\n",
    "        return section_data\n",
    "\n",
    "    def get_candidate_info(self, title):\n",
    "        # To store Candidate information\n",
    "        candidate_info_details = {}\n",
    "        data = self.nlp(self.section_data[title])\n",
    "\n",
    "        # Adding all the patterns to the Matcher to retrieve the corresponding details\n",
    "        for index, info in enumerate(ResumeParser.CANDIDATE_INFO):\n",
    "            self.matcher.add(info['id'], info['match_on'], info['pattern'])\n",
    "            candidate_info_details[info['id']] = 'Null'\n",
    "\n",
    "        # Extracting the details from document text\n",
    "        matches = self.matcher(data)\n",
    "\n",
    "        # Iterating the result set and store the information\n",
    "        for match_id, start, end in matches:\n",
    "            rule_id = str(self.nlp.vocab.strings[match_id])\n",
    "            if candidate_info_details[rule_id] == 'Null':\n",
    "                span = self.doc[start:end]\n",
    "                candidate_info_details[rule_id] = str(span.text)\n",
    "\n",
    "        for key in candidate_info_details:\n",
    "           print(key + \" : \" + candidate_info_details[key])\n",
    "\n",
    "        return {title: candidate_info_details}\n",
    "\n",
    "    def get_summary_text(self, title):\n",
    "        data = self.section_data[title]\n",
    "        data = None if data is None else re.sub(r\"\\s+\", \" \", data).strip()\n",
    "        return {title : data}\n",
    "\n",
    "    def get_work_experience(self, title):\n",
    "        data = self.section_data[title]\n",
    "        data = None if data is None else list(filter(None, data.split('\\n\\n\\n')))\n",
    "        for i, d in enumerate(data):\n",
    "            print('{}----{}'.format(i,re.sub(r\"\\s+\", \" \", d).strip()))\n",
    "        return {title : data}\n",
    "\n",
    "    def parse_information(self):\n",
    "        details = self.get_candidate_info(self.SECTION_TITLE[0])\n",
    "        details.update(self.get_summary_text(self.SECTION_TITLE[1]))\n",
    "        details.update(self.get_work_experience(self.SECTION_TITLE[3]))\n",
    "        return details\n",
    "\n",
    "\n",
    "def main():\n",
    "    rp = ResumeParser('/home/chris/reesby/reverse_Malih/Accountant_Sydney/AudreyPritchardResume-latest.docx')\n",
    "    print(rp.parse_information())\n",
    "    #rp = ResumeParser('./Sample Data/Deepak Kumar_Business Analyst_PM-NC.docx')\n",
    "    #print(rp.parse_information())\n",
    "    #rp = ResumeParser('./Sample Data/Manish_ARORA_Profile.docx')\n",
    "    #print(rp.parse_information())\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e092e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is it a valid number 0414-559-449\n"
     ]
    }
   ],
   "source": [
    "import phonenumbers\n",
    "\n",
    "text='0414-559-449'\n",
    "my_number = phonenumbers.parse(text,\"AU\")\n",
    "print('is it a valid number', list(iter(phonenumbers.PhoneNumberMatcher(text, \"AU\")))[0].raw_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3b00765",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-13d500c07553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# load pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# load pre-trained model\n",
    "base_path = os.path.dirname(__file__)\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "custom_nlp2 = spacy.load(os.path.join(base_path,\"degree\",\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e262870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d07ae4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_subtree(token, dep):\n",
    "    deps =[child.dep_ for child in token.children]\n",
    "    child=next(filter(lambda c: c.dep_==dep, token.children), None)\n",
    "    if child != None:\n",
    "        return \" \".join([c.text for c in child.subtree])\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# to remove citations, e.g. \"[91]\" as this makes problems with spaCy\n",
    "p = re.compile(r'\\[\\d+\\]')\n",
    "\n",
    "def extract_events_spacy(line):\n",
    "    line=p.sub('', line)\n",
    "    events = []\n",
    "    doc = nlp(line)\n",
    "    for ent in filter(lambda e: e.label_=='DATE',doc.ents):\n",
    "        try:\n",
    "            start,end = parse(ent.text)\n",
    "        except:\n",
    "      # could not parse the dates, hence ignore it\n",
    "            continue\n",
    "        current = ent.root\n",
    "        while current.dep_ != \"ROOT\":\n",
    "            current = current.head\n",
    "        desc = \" \".join(filter(None,[\n",
    "                                 dep_subtree(current,\"nsubj\"),\n",
    "                                 dep_subtree(current,\"nsubjpass\"),\n",
    "                                 dep_subtree(current,\"auxpass\"),\n",
    "                                 dep_subtree(current,\"amod\"),\n",
    "                                 dep_subtree(current,\"det\"),\n",
    "                                 current.text, \n",
    "                                 dep_subtree(current,\"acl\"),\n",
    "                                 dep_subtree(current,\"dobj\"),\n",
    "                                 dep_subtree(current,\"attr\"),\n",
    "                                 dep_subtree(current,\"advmod\")]))\n",
    "        events = events + [(start,ent.text,desc)]\n",
    "    return events\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d182987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_events_spacy(\"The Protestant Reformation was the first successful challenge to the Catholic Church and began in 1521 as Luther was outlawed at the Diet of Worms after his refusal to repent. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2a243ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e321ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['February 2020', 'daily', 'May 2019', 'Monthly', 'August 2018', 'May 2019', 'Monthly', '2011-2018', 'these years', 'Nov 2017', 'May 2017', 'Nov 2017', 'May 2017', 'daily', 'Mar 2016', 'Monthly', 'Jan 2012', 'Nov 2012', 'Feb 2010', 'Monthly', 'Monthly', 'Jan 2006', 'Feb 2010 - Hastings Deering', '14 days']\n"
     ]
    }
   ],
   "source": [
    "docx_path = '/home/chris/reesby/reverse_Malih/new_resumes/samuel allen Online Sales Representative Resume.docx'\n",
    "names = extract_text_from_docx(docx_path)\n",
    "#names = 'The Protestant Reformation was the first successful challenge to the Catholic Church and began in 1521 as Luther was outlawed at the Diet of Worms after his refusal to repent. '\n",
    "nlp_dates = nlp(names)\n",
    "        \n",
    "dates = [ent.text for ent in nlp_dates.ents if ent.label_ == 'DATE']\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d383e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "547a7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =';;;;stormy_chris_1@yahoo.com.au; ;;;; bggg@ccc.nen.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4de1358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "email = re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f198018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stormy_chris_1@yahoo.com.au\n"
     ]
    }
   ],
   "source": [
    "print(email[0].split()[0]) #.strip(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fc0b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\n",
      " \n",
      "\n",
      "MENGLY BUN \n",
      "\n",
      " \n",
      " \n",
      "Mobile: 0466 882 818  \n",
      "Email: amengly@gmail.com \n",
      "Linkedin: https://www.linkedin.com/in/mengly-bun-3bb905138/ \n",
      " \n",
      "\n",
      "CAREER OBJECTIVE  \n",
      "\n",
      "An accounting with commerce undergraduate seeking employment with an organization, \n",
      "where business success through moral operation is my purpose. Professional customer \n",
      "service trained through consistent face to face and digital interaction have developed through \n",
      "past experience that has strengthened my interpersonal and team working skills.  \n",
      " \n",
      "I pride myself on being: hard-working, reliable, committed organized and enthusiastic, \n",
      "willing to learn new skills and developing my knowledge and expertise in various functions.    \n",
      " \n",
      "Knowledgeable accounting in bookkeeping and data entry in small business; I gain \n",
      "experience with the applications and the financial reports also the GST calculation. I am still \n",
      "continuous improving my understanding of accounting skill in my third year at the \n",
      "University.   \n",
      " \n",
      "\n",
      "July 2019 – January 2021 \n",
      "\n",
      "March 2017 – June 2019 \n",
      " \n",
      "\n",
      "                       December 2015 – November 2016 \n",
      "\n",
      "   March 2013 – July 2015  \n",
      "\n",
      "            January 2009 – August 2014 \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "EDUCATION  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Bachelor of Commerce (Majoring Accounting) \n",
      "Macquarie University  \n",
      " \n",
      "Bachelor of Business   \n",
      "University of Technology Sydney (UTS) \n",
      " \n",
      "Advance Diploma of Business \n",
      "TAFE South Western Sydney Institute   \n",
      " \n",
      "General English Program \n",
      " Australian Centre of Education \n",
      " \n",
      "High School Certificate in Cambodia  \n",
      "Beoung Trabek High School \n",
      " \n",
      "CAREER HISTORY \n",
      " \n",
      "▪ \n",
      "Passenger Service Agent  \n",
      "Dnata Ground Handling Industry –Sydney  \n",
      "Key Responsibilities \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "January 2018 – Current  \n",
      "\n",
      "o  Deliver a high standard of customer services for client airlines with an efficient and \n",
      "\n",
      "professional manner  \n",
      "\n",
      "\f",
      "\t\n",
      "\n",
      " \n",
      "\n",
      "o  Provide passenger check in duties and other duties with the passenger service and \n",
      "\n",
      "dispatch for customer airlines  \n",
      "\n",
      "o  Lead and participate as a team member to ensure work and company requirements are \n",
      "\n",
      "requirements are met with accuracy and on time procedures \n",
      "\n",
      "o  Resolve problems, liaise with internal and external clients and management required \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    August 2017 –November 2017 \n",
      "\n",
      " \n",
      "Part time –All-rounder  \n",
      "\n",
      "▪ \n",
      " \n",
      "The Emporium coffee shop - Sydney  \n",
      "Key Responsibilities  \n",
      " \n",
      "\n",
      "o  Serving drink to customer at the register  \n",
      "o  Ensuring that customers receive pleasant service and food \n",
      "o  Have a hand on making drink and coffee  \n",
      "o  General cleaning duty  \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "EXPERIENCES \n",
      " \n",
      "Social media reporter (Volunteer) \n",
      "ASEAN Youth Organisation– Cambodia \n",
      "\n",
      " \n",
      "\n",
      "Will Day event \n",
      "\n",
      " \n",
      "Event Management (Volunteer)  \n",
      "BLUNIC in the BOPHEA MUSIC FESTIVAL \n",
      " \n",
      "Key Responsibilities \n",
      " \n",
      "\n",
      "o  Promoting the event and members in the logistic team \n",
      "o  Mentoring the Traditional Instrument Exhibition  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o  Managing the pages activities and contents \n",
      "o  Representing the ASEAN Community Cambodia Judge of photo contest on the Green \n",
      "\n",
      "  \n",
      "\n",
      "        January 2014 \n",
      "\n",
      "                November 2014 \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "May 2020 – Current  \n",
      "\n",
      "o  Being mentor by the manager business system and application (Konica Minolta \n",
      "\n",
      "o  Work together on small work-based activities such as attending mock interviews, and \n",
      "\n",
      "o  Learning fresh perspectives and ways of working from each other \n",
      "o  Developed team communications and information for mentoring and difference topics \n",
      "\n",
      "Mentee Student  \n",
      "\n",
      " \n",
      "▪ \n",
      "Lucy Mentoring Program  \n",
      "Key Responsibilities  \n",
      " \n",
      "\n",
      "Australia) \n",
      "\n",
      "client meetings \n",
      "\n",
      "around business meetings. \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "PERSONLA SKILLS  \n",
      "\n",
      "\f",
      "Technical Skills \n",
      " \n",
      "\n",
      "o  Proficient in Applications \n",
      "o  MYOB \n",
      "o  XERO \n",
      "o  Touchtype  \n",
      "o  Photography \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "Languages  \n",
      " \n",
      "\n",
      " \n",
      "Teamwork and Communication \n",
      "\n",
      "o  Strong English and Fluent on Khmer –Cambodian (Native language)  \n",
      "\n",
      "o  Strong communication with colleague and clients both internal and external  \n",
      "o  Good at solving the critical problem and deal with difficult clients using persuasive \n",
      "\n",
      "skills and politeness \n",
      "\n",
      "o  Competence team player with an interest in achieving a goal within the timeframe and \n",
      "\n",
      "coordinating schedule with the team member \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "      March 2017 –Current  \n",
      "\n",
      "October 2020 – Current \n",
      "\n",
      " \n",
      "EXTRA-CURRICULAR ACTIVITIES  \n",
      " \n",
      "Team Participant  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o  Macquarie University Business Society   \n",
      "o  Macquarie Global Leadership Program  \n",
      "o  UTS Cambodian Society  \n",
      "o  UTS Photography Society  \n",
      "\n",
      " \n",
      "Participant & Charity Event Fundraiser    \n",
      "\n",
      "▪ \n",
      "Wayside Chapel Long Walk Home – Sydney  \n",
      "Key Responsibilities  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o  Pledge to walk 28 km \n",
      "o  Self-Donation & Get Sponsors \n",
      "o  Setting Goal of $500 \n",
      "o  Reached the raise of $30  \n",
      "o  Communicate with the participant and join the group  \n",
      "\n",
      " \n",
      " \n",
      "PERSONAL INTERESTS \n",
      "\n",
      "o  Exercise (swimming, jogging, cycling…) \n",
      "o  Learning different languages (Korean, French, etc)  \n",
      "o  Working as adventure team \n",
      "o  Capture the great image \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\t\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_text = extract_text_from_pdf('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff3d1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(pdf_file):\n",
    "    \"\"\"\n",
    "    A utility function to convert a machine-readable PDF to raw text.\n",
    "\n",
    "    This code is largely borrowed from existing solutions, and does not match the style of the rest of this repo.\n",
    "    :param input_pdf_path: Path to the .pdf file which should be converted\n",
    "    :type input_pdf_path: str\n",
    "    :return: The text contents of the pdf\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # try:\n",
    "        # PDFMiner boilerplate\n",
    "        # pdf = pdfplumber.open(pdf_file)\n",
    "        # full_string= \"\"\n",
    "        # for page in pdf.pages:\n",
    "        #   full_string += page.extract_text() + \"\\n\"\n",
    "        # pdf.close()\n",
    "\n",
    "        \n",
    "    try:\n",
    "        raw_text = parser.from_file(pdf_file, service='text')['content']\n",
    "    except RuntimeError as e:            \n",
    "        #logging.error('Error in tika installation:: ' + str(e))\n",
    "        #logging.error('--------------------------')\n",
    "        #logging.error('Install java for better result ')\n",
    "        pdf = pdfplumber.open(pdf_file)\n",
    "        raw_text= \"\"\n",
    "        for page in pdf.pages:\n",
    "          raw_text += page.extract_text() + \"\\n\"\n",
    "        pdf.close()                \n",
    "    except Exception as e:\n",
    "        #logging.error('Error in docx file:: ' + str(e))\n",
    "        return [], \" \"\n",
    "    try:\n",
    "        full_string = re.sub(r'\\n+', '\\n', raw_text)\n",
    "        full_string = full_string.replace(\"\\r\", \"\\n\")\n",
    "        full_string = full_string.replace(\"\\t\", \" \")\n",
    "\n",
    "        # Remove awkward LaTeX bullet characters\n",
    "\n",
    "        full_string = re.sub(r\"\\uf0b7\", \" \", full_string)\n",
    "        full_string = re.sub(r\"\\(cid:\\d{0,2}\\)\", \" \", full_string)\n",
    "        full_string = re.sub(r'• ', \" \", full_string)\n",
    "\n",
    "        # Split text blob into individual lines\n",
    "        resume_lines = full_string.splitlines(True)\n",
    "\n",
    "        # Remove empty strings and whitespaces\n",
    "        resume_lines = [re.sub('\\s+', ' ', line.strip()) for line in resume_lines if line.strip()]\n",
    "\n",
    "        return resume_lines, raw_text\n",
    "    except Exception as e:\n",
    "        #logging.error('Error in docx file:: ' + str(e))\n",
    "        return [], \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2f22025",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = convert_pdf_to_txt('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d3821e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ' ')\n"
     ]
    }
   ],
   "source": [
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d063aec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_experience' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f1a91fa73de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_experience' is not defined"
     ]
    }
   ],
   "source": [
    "exp = calculate_experience(resume_text)\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ec6c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "def extract_experience(resume_text):\n",
    "    '''\n",
    "    Helper function to extract experience from resume text\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: list of experience\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization \n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and wordnet_lemmatizer.lemmatize(w) not in stop_words] \n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "    \n",
    "    # for i in cs.subtrees(filter=lambda x: x.label() == 'P'):\n",
    "    #     print(i)\n",
    "    \n",
    "    test = []\n",
    "    \n",
    "    for vp in list(cs.subtrees(filter=lambda x: x.label()=='P')):\n",
    "        test.append(\" \".join([i[0] for i in vp.leaves() if len(vp.leaves()) >= 2]))\n",
    "\n",
    "    # Search the word 'experience' in the chunk and then print out the text after it\n",
    "    x = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 'experience' in x.lower()]\n",
    "    print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81b97a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-18b87c854545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-befbfe18c61f>\u001b[0m in \u001b[0;36mextract_experience\u001b[0;34m(resume_text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# word tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# remove stop words and lemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     return [\n\u001b[1;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m    107\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \"\"\"\n\u001b[1;32m   1358\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "extract_experience(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916949b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb9b4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "#doc = docx.Document(\"/home/chris/Documents/Chris Matthews-Resume.docx\")\n",
    "doc = docx.Document('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Anthony-Scriba-Resume-2020-v4.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96224407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paras = doc.paragraphs\n",
    "len(all_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fba63ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony Scriba\n",
      "-------\n",
      "Operations Administrator\n",
      "-------\n",
      "Melbourne, Australia +61 402 419 377  \n",
      "-------\n",
      "\n",
      "-------\n",
      "Profile               \n",
      "-------\n",
      "\n",
      "-------\n",
      "\n",
      "-------\n",
      "A results-driven and highly motivated administrator with experience leading the analytical, billing and reporting functions in fast moving, high volume environments across the telecommunications industry. Uses a combination of strong stakeholder relationship building skills, commercial focus and technical expertise to deliver accurate and timely support services to the Supply Chain department.\n",
      "-------\n",
      "\n",
      "-------\n",
      "Key Strengths               \n",
      "-------\n",
      "\n",
      "-------\n",
      "Analysis +\t\t\n",
      "-------\n",
      "Billing\t\t\n",
      "-------\n",
      "Supply Chain + \n",
      "-------\n",
      "Logistics\n",
      "-------\n",
      "Significant experience working alongside 3PL providers to streamline process and billing support in managing inventory, freight and warehouse\n",
      "-------\n",
      "Grasp of essential supply chain drivers to lead efficiencies and deliver cost savings through data analysis\n",
      "-------\n",
      "\n",
      "-------\n",
      "Procurement\n",
      "-------\n",
      "Builds trusting, collaborative, ongoing relationships with diverse internal and external stakeholders, using strong interpersonal skills to influence and negotiate\n",
      "-------\n",
      "Provide end to end solution from purchase requisition to validating purchases against customer billing\n",
      "-------\n",
      "Stakeholder \n",
      "-------\n",
      "Engagement + \n",
      "-------\n",
      "Communication\n",
      "-------\n",
      "Builds trusting, collaborative, ongoing relationships with diverse internal and external stakeholders, using strong interpersonal skills to influence and negotiate\n",
      "-------\n",
      "Prepares quality written communications including spend of incoming and outgoing and internal communications.\n",
      "-------\n",
      "\n",
      "-------\n",
      "Key Achievement               \n",
      "-------\n",
      "\n",
      "-------\n",
      "Highly focused and established analytical subject matter expert with solid problem solving and understanding of supply chain drivers in areas pertaining to the upkeep and reconciliation of Optus’ billing and reporting. \n",
      "-------\n",
      "\n",
      "-------\n",
      "Career History           \n",
      "-------\n",
      "\n",
      "-------\n",
      "OPERATIONS ADMINISTRATOR/SUPPLY CHAIN\n",
      "-------\n",
      "Optus | MAY 2017 – Current\n",
      "-------\n",
      "Tracking Spend and transaction volumes daily, weekly and monthly operational performance reporting and auditing for all major Device Vendors, Optus Insurance Service, DOA/ELF and Freight & Warehouse.\n",
      "-------\n",
      "Perform fault finding and system analysis for issues relating to manufacturers and devices and vendor spend.\n",
      "-------\n",
      "Manage, reconcile and process all incoming invoices and reporting for the aftersales Program (Apple, Samsung, 3PL, Repair Agents) ($60M per annum spend).\n",
      "-------\n",
      "Responsible for Optus Insurance Service (OIS) aftersales program revenue ($30M) \n",
      "-------\n",
      "Purchasing handsets and spare parts for the Mobile Service department including inventory management using SAP\n",
      "-------\n",
      "Manual and Automated Customer, Franchise and Internal billing for SWAP, (SUR) Same Unit Repair, Lease and Loan Programs and incoming revenue.\n",
      "-------\n",
      "Audit, investigate and reconcile payment leakages.\n",
      "-------\n",
      "Auditing all incoming/outgoing Accounts Payable payment transactions to ensure Optus is fully reimbursed for parts and labour.\n",
      "-------\n",
      "Auditing courier accounts for unauthorised misuse in incoming/outgoing shipments.\n",
      "-------\n",
      "Strong Customer Service / Relationship Management (Internal/External), Manufacturers / Handset Vendors, Key financial and AP vendor functions & 3PL Vendors.\n",
      "-------\n",
      "\n",
      "-------\n",
      "Key Achievements\n",
      "-------\n",
      "Commissioned 3PL invoicing and reporting solely with new provider (Assurant)\n",
      "-------\n",
      "Developed internal changes to systems, billing and claims processes to automate rather than manual workarounds.\n",
      "-------\n",
      "Introduced vendor invoice process which consolidated over 200 daily invoices into 1.\n",
      "-------\n",
      "Identified vendor overcharges of over $1M through reconciliation and audit which was highlighted to senior management for recovery \n",
      "-------\n",
      "Commissioned tracking system to reconcile vendor charges to customer billing which validated no financial leakages\n",
      "-------\n",
      "Developed freight and warehouse tracker to analyse P/L ($40M in freight and warehouse spend)\n",
      "-------\n",
      "\n",
      "-------\n",
      "SERVICE ADMINISTRATOR/AFTER SALES\n",
      "Optus | FEB 2008 – May 2017\n",
      "-------\n",
      "Administer daily, weekly and monthly reporting and auditing for all major manufacturer warranty claiming, Optus Insurance and DOA/ELF Boomerang.\n",
      "-------\n",
      "Perform fault finding and system analysis for issues relating to manufacturers and devices.\n",
      "-------\n",
      "Reconcile and process all incoming invoices for the department. \n",
      "-------\n",
      "Parts purchaser for after sales service from a variety of Manufactures including forecasting.\n",
      "-------\n",
      "Purchasing handset stock for the Mobile Service department.\n",
      "-------\n",
      "Responsible for processing invoices for over $1 Million dollars per month for services rendered by after sale service.\n",
      "-------\n",
      "Managing the manufacturer warranty and DOA/ELF (Boomerang) process for all major manufacturers supported by Optus. \n",
      "-------\n",
      "Data analysis and reporting.\n",
      "-------\n",
      "Investigation and reconciliation on potential payment leakages.\n",
      "-------\n",
      "Auditing all incoming/outgoing payment transactions to ensure Optus is fully reimbursed for parts and labour.\n",
      "-------\n",
      "Auditing courier accounts for unauthorised misuse by Optus dealers in incoming/outgoing shipments\n",
      "-------\n",
      "Develop internal processes for systems, claiming changes and new business.\n",
      "-------\n",
      "\n",
      "-------\n",
      "SERVICE REPRESENTATIVE/MOBILE SEVICE TEAM\n",
      "-------\n",
      "Optus | AUG 2006 – JAN 2008\n",
      "-------\n",
      "Demonstrate high levels of customer service with all internal and external customers including Dealers and \n",
      "-------\n",
      "Outlet staff, subcontractors.\n",
      "-------\n",
      "Complete daily book-in and job receipt of all repairs from outlets and channels.\n",
      "-------\n",
      "Develop new process for simplifying lengthy tasks\n",
      "-------\n",
      "Process all outgoing subcontractor work back to Optus outlets and all insurance work.\n",
      "-------\n",
      "Handle Call Centre enquires, escalations and ensure all incoming phone calls are answered within the KPI target.\n",
      "-------\n",
      "Sort incoming repairs, accurately record and monitor all incoming volumes including subcontractor work.\n",
      "-------\n",
      "Support the repair technicians in level 1 repairs and fault identification\n",
      "-------\n",
      "\n",
      "-------\n",
      "PRODUCT SPECIALIST/OPTUS INSURANCE\n",
      "-------\n",
      "SERVICE STREAM SOLUTIONS | MAR 2004 – AUG 2006\n",
      "-------\n",
      "Member of Special Assessment Team and tasked with investigating fraudulent Insurance Claims.\n",
      "-------\n",
      "Conduct IMEI traces to confirm or deny customer claims and contact customer to provide approval or denial and reasoning.\n",
      "-------\n",
      "Dealing with customer escalations.\n",
      "-------\n",
      "Training new staff on customer claim processes, systems, and call handling techniques.\n",
      "-------\n",
      "Product Specialist for Optus Insurance.\n",
      "-------\n",
      "Recommend replacement handsets for discontinued phones and present to Optus for authorisation.\n",
      "-------\n",
      "Handle technical queries relating to handsets and accessories.\n",
      "-------\n",
      "Updating of the Optus Insurance phone guide, creating monthly handset pricelists and distributing. \n",
      "-------\n",
      "Monitoring the Call Centre queue and staff queries to ensure we meet our target SLA’s and KPI’s. \n",
      "-------\n",
      "\n",
      "-------\n",
      "Training & Education         \n",
      "-------\n",
      "\n",
      "-------\n",
      "Desktop - MS Excel – Level 1 & Level 2 - New Horizons Computer Learning Centre\n",
      "-------\n",
      "Certificate IV in Assessment and Workplace Training\n",
      "Project Supervisor-Key Solutions..\n",
      "-------\n",
      "IT/Web Design Project, Key Solutions, Melbourne\n",
      "HTML/Web Design, Macromedia Dream Weaver 5/MX, Flash 5/MX, Fire Works MX. Adobe Photoshop 7.0. Microsoft FrontPage.\n",
      "IT/Web Design Project, Mission Australia, Melbourne\n",
      "HTML/Web Design, Java Programming. Macromedia Dream Weaver 4/MX, Flash 5/MX, Fire Works 4. Adobe Photoshop 5.0, 6.0, 7.0. Microsoft FrontPage. Light Wave 3D 7.0.\n",
      "-------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced HTML Web Design Course, NMIT, Melbourne\n",
      "Advanced HTML, Java, CGI Scripts. Macromedia Dream Weaver 4, Flash 5, Fire Works 4. Adobe Photoshop 5.0, 6.0. Microsoft FrontPage\n",
      "e-commerce Course, IBM Australia, Melbourne\n",
      "HTML - Design and building of a website. Macromedia Dream Weaver. Introduction to Java. Project Planning  \n",
      "-------\n",
      "Victorian Certificate of Education, Redden Catholic College, Preston\n",
      "\n",
      "-------\n",
      "\n",
      "-------\n",
      "Technical Skills\n",
      "-------\n",
      "\n",
      "Optus Applications: SAP CMS, Boomerang, Pronto, Jarvis , GSMIS, Arbor, OPOM, Focus, IBS, Smart.\n",
      "-------\n",
      "Microsoft Operating Systems: Windows 3.1, 95, 98, 98SE, 2000, ME, XP Home, XP Pro, Longhorn, Vista. Windows 7 & 10.\n",
      "Microsoft Software: Outlook, Excel FrontPage, Word, PowerPoint, Publisher, Visio, Onwards, Versions 2002 to 365.\n",
      "-------\n",
      "SAP:  SAP CMS – Accounts Payable - Purchase Orders/Good Receiving, Inventory and Purchasing.\n",
      "Macromedia Software: Dream Weaver, Flash, Fireworks MX, Free Hand MX, Director MX, Ultra Dev.\n",
      "-------\n",
      "Apple: MAC OSX, IOS 3.0-14.0.\n",
      "Adobe Software: Photoshop, Acrobat, Reader, Elements, Illustrator, Premiere, Streamline, Dimensions, \n",
      "Animation: Light Wave 3D 7.0, 7.5, Animation Shop.\n",
      "Java: Basic Knowledge of Java Programming.\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "-------\n",
      "Interests           \n",
      "\n",
      "I enjoy technology, animation, movies/cinema, cooking, gardening, music, and Volunteering. I also enjoy many sports including soccer, gridiron, and AFL.\n",
      "-------\n",
      "\n",
      "-------\n",
      "\n",
      "-------\n",
      "References           \n",
      "-------\n",
      "\n",
      "Available on request\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for para in all_paras:\n",
    "    print(para.text)\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "996f2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "worked = ['Dr Oertker Queen Fine Foods- 2018-Current', 'Service Operator/Picker and Packer', 'Cleaning and assisting with maintenance of production', 'equipment and machinery', 'of orders accurately and suitably for shipping', 'operate on main and subsidiary production lines', 'Working in conjunction with strict OH&S guidelines and', 'conforming to working in', 'sterile environments', 'Daily stock counting and quality control', 'Correct manual handling procedures in a fast-paced', 'environment', 'Manly Bar & Hotel 2016-2018', 'Bar Attendant and Customer Service', 'Phone and walk in enquiries', 'Restaurant service and waiting', 'Maintain a high standard of presentation, work ethic and', 'professionalism', 'Consolidating, updating and carrying out stocktakes', 'CSI (Club Southport Incorporated) 2014-2016', 'Frontline Administration', 'Staff timetables and schedules', 'Appointment scheduling and table reservation management', 'Management of functions and events', 'Competition implementation and management', 'Bar and restaurant management', 'Handle phone and walk in enquiries', 'Maintain customer membership logs and members club', 'Maintain a high standard of presentation, work ethic and', 'professionalism', 'Lucas Papaw Remedies 2011-2012', 'General Warehousing Order Management', 'Storing and shipping delicate items such as pharmaceuticals.', 'Following detailed instructions and meeting specific', 'customer requirements.', 'Invoicing, data entry and general office administration.', 'REFERENCES', 'Richard Song', 'Production manager', 'Dr Oertker Queen Fine Foods', '0430119772', 'Francine Anderson', 'Team Leader', 'Dr Oertker Queen Fine Foods', '0412505944', 'Dynamic Supplies 2010-2011', 'Pick Packer', 'Consolidating, updating and carrying out stocktakes', 'Ensuring timely picking and packing of orders are kept and', 'meeting stringent', 'warehouse production standards.', 'Ensuring all equipment is kept and maintained in exceptional', 'working', 'condition and to ensure preventive maintenance was carried', 'out according to schedule.', 'Snap Fresh 2008-2010', 'Process Worker', 'Organising, preparing food for packaging, maintaining', 'workplace hygiene,', 'cleaning equipment.', 'Made sure all stations in the kitchen were clean and up to OH', '& S standard', 'Preparing food orders for despatch', 'Checked stock levels, expiry dates and managed any out of', 'date food.']\n",
    "worked_at = ' '.join(worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86459ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 2018-Current CARDINAL\n",
      "image OH&S ORG\n",
      "image Daily DATE\n",
      "image Manly Bar & Hotel 2016-2018 ORG\n",
      "image Bar Attendant PERSON\n",
      "image Customer Service Phone ORG\n",
      "image CSI ORG\n",
      "image 2014 DATE\n",
      "image Frontline Administration Staff ORG\n",
      "image Management of functions ORG\n",
      "image Bar ORG\n",
      "image Handle ORG\n",
      "image Lucas Papaw Remedies PERSON\n",
      "image General Warehousing Order Management Storing ORG\n",
      "image Richard Song PERSON\n",
      "image Francine Anderson Team ORG\n",
      "image 2010-2011 DATE\n",
      "image Pick Packer Consolidating PERSON\n",
      "image OH & S ORG\n",
      "image Checked PERSON\n"
     ]
    }
   ],
   "source": [
    "sent_text = nltk.sent_tokenize(worked_at)\n",
    "for sen in sent_text:\n",
    "    tags = nlp(sen)\n",
    "    for ent in tags.ents:\n",
    "        print(word,ent.text, ent.label_)\n",
    "        #if(tags.ent =='DATE'):\n",
    "        #    print(sen)\n",
    "    #print(sen)\n",
    "    #word_tokens = nltk.word_tokenize(sen)\n",
    "    #for word in word_tokens:\n",
    "    #    tags = nlp(word)\n",
    "    #    for ent in tags.ents:\n",
    "    #        print(word,ent.text, ent.label_) # ent.start_char, ent.end_char,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96362887",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = (\n",
    "        'Interests',\n",
    "        'Key Achievement',\n",
    "        'career goal',\n",
    "        'objective',\n",
    "        'career objective',\n",
    "        'employment objective',\n",
    "        'professional objective',\n",
    "        'summary',\n",
    "        'career summary',\n",
    "        'professional summary',\n",
    "        'summary of qualifications'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdb51f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply get Section title headings from resume based on list of possible titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b72f6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\n",
      " \n",
      "\n",
      "MENGLY BUN \n",
      "\n",
      " \n",
      " \n",
      "Mobile: 0466 882 818  \n",
      "Email: amengly@gmail.com \n",
      "Linkedin: https://www.linkedin.com/in/mengly-bun-3bb905138/ \n",
      " \n",
      "\n",
      "CAREER OBJECTIVE  \n",
      "\n",
      "An accounting with commerce undergraduate seeking employment with an organization, \n",
      "where business success through moral operation is my purpose. Professional customer \n",
      "service trained through consistent face to face and digital interaction have developed through \n",
      "past experience that has strengthened my interpersonal and team working skills.  \n",
      " \n",
      "I pride myself on being: hard-working, reliable, committed organized and enthusiastic, \n",
      "willing to learn new skills and developing my knowledge and expertise in various functions.    \n",
      " \n",
      "Knowledgeable accounting in bookkeeping and data entry in small business; I gain \n",
      "experience with the applications and the financial reports also the GST calculation. I am still \n",
      "continuous improving my understanding of accounting skill in my third year at the \n",
      "University.   \n",
      " \n",
      "\n",
      "July 2019 – January 2021 \n",
      "\n",
      "March 2017 – June 2019 \n",
      " \n",
      "\n",
      "                       December 2015 – November 2016 \n",
      "\n",
      "   March 2013 – July 2015  \n",
      "\n",
      "            January 2009 – August 2014 \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "EDUCATION  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Bachelor of Commerce (Majoring Accounting) \n",
      "Macquarie University  \n",
      " \n",
      "Bachelor of Business   \n",
      "University of Technology Sydney (UTS) \n",
      " \n",
      "Advance Diploma of Business \n",
      "TAFE South Western Sydney Institute   \n",
      " \n",
      "General English Program \n",
      " Australian Centre of Education \n",
      " \n",
      "High School Certificate in Cambodia  \n",
      "Beoung Trabek High School \n",
      " \n",
      "CAREER HISTORY \n",
      " \n",
      "▪ \n",
      "Passenger Service Agent  \n",
      "Dnata Ground Handling Industry –Sydney  \n",
      "Key Responsibilities \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "January 2018 – Current  \n",
      "\n",
      "o  Deliver a high standard of customer services for client airlines with an efficient and \n",
      "\n",
      "professional manner  \n",
      "\n",
      "\f",
      "\t\n",
      "\n",
      " \n",
      "\n",
      "o  Provide passenger check in duties and other duties with the passenger service and \n",
      "\n",
      "dispatch for customer airlines  \n",
      "\n",
      "o  Lead and participate as a team member to ensure work and company requirements are \n",
      "\n",
      "requirements are met with accuracy and on time procedures \n",
      "\n",
      "o  Resolve problems, liaise with internal and external clients and management required \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    August 2017 –November 2017 \n",
      "\n",
      " \n",
      "Part time –All-rounder  \n",
      "\n",
      "▪ \n",
      " \n",
      "The Emporium coffee shop - Sydney  \n",
      "Key Responsibilities  \n",
      " \n",
      "\n",
      "o  Serving drink to customer at the register  \n",
      "o  Ensuring that customers receive pleasant service and food \n",
      "o  Have a hand on making drink and coffee  \n",
      "o  General cleaning duty  \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "EXPERIENCES \n",
      " \n",
      "Social media reporter (Volunteer) \n",
      "ASEAN Youth Organisation– Cambodia \n",
      "\n",
      " \n",
      "\n",
      "Will Day event \n",
      "\n",
      " \n",
      "Event Management (Volunteer)  \n",
      "BLUNIC in the BOPHEA MUSIC FESTIVAL \n",
      " \n",
      "Key Responsibilities \n",
      " \n",
      "\n",
      "o  Promoting the event and members in the logistic team \n",
      "o  Mentoring the Traditional Instrument Exhibition  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o  Managing the pages activities and contents \n",
      "o  Representing the ASEAN Community Cambodia Judge of photo contest on the Green \n",
      "\n",
      "  \n",
      "\n",
      "        January 2014 \n",
      "\n",
      "                November 2014 \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "May 2020 – Current  \n",
      "\n",
      "o  Being mentor by the manager business system and application (Konica Minolta \n",
      "\n",
      "o  Work together on small work-based activities such as attending mock interviews, and \n",
      "\n",
      "o  Learning fresh perspectives and ways of working from each other \n",
      "o  Developed team communications and information for mentoring and difference topics \n",
      "\n",
      "Mentee Student  \n",
      "\n",
      " \n",
      "▪ \n",
      "Lucy Mentoring Program  \n",
      "Key Responsibilities  \n",
      " \n",
      "\n",
      "Australia) \n",
      "\n",
      "client meetings \n",
      "\n",
      "around business meetings. \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "PERSONLA SKILLS  \n",
      "\n",
      "\f",
      "Technical Skills \n",
      " \n",
      "\n",
      "o  Proficient in Applications \n",
      "o  MYOB \n",
      "o  XERO \n",
      "o  Touchtype  \n",
      "o  Photography \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "Languages  \n",
      " \n",
      "\n",
      " \n",
      "Teamwork and Communication \n",
      "\n",
      "o  Strong English and Fluent on Khmer –Cambodian (Native language)  \n",
      "\n",
      "o  Strong communication with colleague and clients both internal and external  \n",
      "o  Good at solving the critical problem and deal with difficult clients using persuasive \n",
      "\n",
      "skills and politeness \n",
      "\n",
      "o  Competence team player with an interest in achieving a goal within the timeframe and \n",
      "\n",
      "coordinating schedule with the team member \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "      March 2017 –Current  \n",
      "\n",
      "October 2020 – Current \n",
      "\n",
      " \n",
      "EXTRA-CURRICULAR ACTIVITIES  \n",
      " \n",
      "Team Participant  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o  Macquarie University Business Society   \n",
      "o  Macquarie Global Leadership Program  \n",
      "o  UTS Cambodian Society  \n",
      "o  UTS Photography Society  \n",
      "\n",
      " \n",
      "Participant & Charity Event Fundraiser    \n",
      "\n",
      "▪ \n",
      "Wayside Chapel Long Walk Home – Sydney  \n",
      "Key Responsibilities  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o  Pledge to walk 28 km \n",
      "o  Self-Donation & Get Sponsors \n",
      "o  Setting Goal of $500 \n",
      "o  Reached the raise of $30  \n",
      "o  Communicate with the participant and join the group  \n",
      "\n",
      " \n",
      " \n",
      "PERSONAL INTERESTS \n",
      "\n",
      "o  Exercise (swimming, jogging, cycling…) \n",
      "o  Learning different languages (Korean, French, etc)  \n",
      "o  Working as adventure team \n",
      "o  Capture the great image \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\t\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_text = extract_text_from_pdf('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca6270a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in all_paras:\n",
    "    if(para in objective):\n",
    "        print(para.text)\n",
    "        print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8be67138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1607072b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resume' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-298e1ff81323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmatch_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'whole_text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'matched_text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcommon_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resume' is not defined"
     ]
    }
   ],
   "source": [
    "d = {'objective': ['career goal','objective','career objective','employment objective','professional objective','summary','career summary',\n",
    "        'professional summary','summary of qualifications','personal statement'],\n",
    "     'work_and_employment':  ['employment history','work history','work experience','experience','professional experience','professional background',\n",
    "        'additional experience','career related experience','related experience','freelance','freelance experience','army experience',\n",
    "        'military experience','military background','employment','career experience','career summary','career history','project management',\n",
    "        'project work','professional proficiency'],\n",
    "     'education_and_training': ['academic background','academic experience','programs','courses','related courses','education',\n",
    "        'educational background','educational qualifications','educational training','education and training','training','academic training',\n",
    "        'professional training','course project experience','related course projects','internship experience','internships','apprenticeships',\n",
    "        'college activities','certifications','special training','qualifications','degree'],\n",
    "     'skills_header': ['credentials','qualifications','areas of experience','areas of expertise','areas of knowledge','skills',\"other skills\",\n",
    "        \"other abilities\",'career related skills','professional skills','specialized skills','technical skills','computer skills',\n",
    "        'personal skills','computer knowledge','technologies','technical experience','proficiencies','languages','language competencies and skills',\n",
    "        'programming languages','competencies'],\n",
    "     'misc' :['activities and honors','activities','affiliations','professional affiliations','associations','professional associations',\n",
    "        'memberships','professional memberships','athletic involvement','community involvement','referee','civic activities',\n",
    "        'extra-curricular activities','professional activities','volunteer work','volunteer experience','additional information',\n",
    "        'interests','volunteer','volunteering','community'],\n",
    "     'accomplishments':['achievement','licenses','presentations','conference presentations','conventions','dissertations','exhibits',\n",
    "        'papers','publications','professional publications','research','research grants','project','research projects','personal projects',\n",
    "        'current research interests','thesis','theses','projects']\n",
    "}\n",
    "\n",
    "words = set(w for lst in d.values() for w in lst)\n",
    "match_stats = {'whole_text': [], 'matched_text': []}\n",
    "for line in resume:\n",
    "    common_words = set(line.split()) & words\n",
    "    if not common_words:\n",
    "        match_stats['whole_text'].append(line)\n",
    "        match_stats['matched_text'].append('NA')\n",
    "    else:\n",
    "        for w in common_words:\n",
    "            match_stats['whole_text'].append(line)\n",
    "            match_stats['matched_text'].append(w)\n",
    "\n",
    "df = pd.DataFrame(match_stats)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25c8ba6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-63e651d4c638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f882259",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = ['career goal','objective','career objective','employment objective','professional objective','summary','career summary',\n",
    "        'professional summary','summary of qualifications','personal statement']\n",
    "work_and_employment=  ['employment history','work history','work experience','experience','professional experience','professional background',\n",
    "        'additional experience','career related experience','related experience','freelance','freelance experience','army experience',\n",
    "        'military experience','military background','employment','career experience','career summary','career history','project management',\n",
    "        'project work','professional proficiency']\n",
    "education_and_training= ['academic background','academic experience','programs','courses','related courses','education',\n",
    "        'educational background','educational qualifications','educational training','education and training','training','academic training',\n",
    "        'professional training','course project experience','related course projects','internship experience','internships','apprenticeships',\n",
    "        'college activities','certifications','special training','qualifications','degree']\n",
    "skills_header= ['credentials','qualifications','areas of experience','areas of expertise','areas of knowledge','skills',\"other skills\",\n",
    "        \"other abilities\",'career related skills','professional skills','specialized skills','technical skills','computer skills',\n",
    "        'personal skills','computer knowledge','technologies','technical experience','proficiencies','languages','language competencies and skills',\n",
    "        'programming languages','competencies']\n",
    "misc=['activities and honors','activities','affiliations','professional affiliations','associations','professional associations',\n",
    "        'memberships','professional memberships','athletic involvement','community involvement','referee','civic activities',\n",
    "        'extra-curricular activities','professional activities','volunteer work','volunteer experience','additional information',\n",
    "        'interests','volunteer','volunteering','community']\n",
    "accomplishments=['achievement','licenses','presentations','conference presentations','conventions','dissertations','exhibits',\n",
    "        'papers','publications','professional publications','research','research grants','project','research projects','personal projects',\n",
    "        'current research interests','thesis','theses','projects']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fd46b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text =  convert_pdf_to_string('/home/chris/reesby/reverse_Malih/Accountant_Sydney/Resume-Document-Mengly-BUN.pdf')\n",
    "#print(output_string.getvalue()) \n",
    "contents = resume_text.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81bc5bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = extract_text_from_docx('/home/chris/reesby/reverse_Malih/Accountant_Sydney/AudreyPritchardResume-latest.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4b4e809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audrey Pritchard\\n\\nSenior Financial Manager \\n\\nSydney, Australia ▪ 0414-559-449\\n\\naagordon@aapt.net.au ▪ LinkedIn\\n\\n\\n\\n\\n\\nSummary\\n\\nCA and CPA qualified professional with extensive experience in the delivery of high-end financial services for FMCG, IT, Media & Publishing, Education, Public Health and NGO’s sectors. Motivated and analytical leader with proven history of establishing and implementing policies and systems to maximise returns on investment, increase efficiency, and achieve organisational objectives whilst complying with recognised accounting standards.\\n\\nQualifications Summary\\n\\nCollaborative communicator with repeated success building relationships and promoting synergy across business lines to drive positive change and cohesive business approaches. \\n\\n\\n\\nProven track record of evaluating organisational and financial performance to determine areas of potential cost reduction and improvement \\n\\n\\n\\nConfident problem solver skilled at improving existing processes and systems and building strong control environments to drive success.\\n\\nCareer Experience\\n\\nAustralian Indigenous Education Foundation August 2019 – January 2020\\n\\nFinance Manager (Business Improvement)\\n\\nOversaw day to day processing of financial transactions as backfill for the Finance Manager. Administered payroll and tax management including GST, FBT and superannuation withholdings. Developed, maintained, and analysed budgets and prepared periodic reports comparing budgeted costs to actual costs. Examined and analysed accounting records, financial statements, and other financial reports to assess accuracy, completeness, and conformance to reporting and procedural standards. Computed taxes owed and prepared tax returns, ensuring compliance with payment, reporting, and other tax requirements. Responsible for accounting for donations in CRM.\\n\\nConducted successful review of ERP system post implementation and recommended modifications resulting in improved GST integrity.\\n\\nSuccessfully planned and implemented several projects from conception to completion including automation of journals and application of Bank Feeds.\\n\\nCreated procedures manual to ensure consistency of operations and compliance.\\n\\nImproved Financial Reporting templates for enhanced reporting.\\n\\n\\n\\nKolling Institute of Medical Research February 2018 – August 2019\\n\\nFinance Manager\\n\\nAdministered day to day accounting activities for research, clinical trials and education portfolios adhering to recognized accounting standards and MOH policy. Provided robust financial and budgetary analysis and reporting for research cost centres. Reviewed accounts for variances, reconciled differences and reported on KPI’s. Created, implemented, and modified financial policies, processes and accounting systems ensuring robust financial management systems and controls minimizing financial and legal risks. Supplied financial reports and advice to institute executives and local health district management. Advised management on issues such as resource utilization, and assumptions underlying budget forecasts to maximise fund performance. Contributed to business planning, developing business cases and briefs. Collaborated with internal and external audits meeting reporting and auditing requirements. Established and maintained effective working relationships with key stakeholders. Responsible for grant management and acquittals. Led a team with one direct report.\\n\\nSignificantly slashed number and value of overdrawn trust funds by $200K through effective cash flow forecasting and provision of sound technical advice to cost centre management.\\n\\nPrepared standing journals resulting in reduction of EOM reporting times to only one day.\\n\\nStreamlined approvals process and educated budget managers on documentary requirements reducing number of transactions delayed or rejected.\\n\\nCollaborated with cost centre managers, clinicians, and researchers to complete 300 trust funds, 7 general funds and 33 clinical trial budgets within the MOH deadlines.\\n\\nConducted accurate budget variance analysis for institute executives within 3 days highlighting issues and making recommendations for improvements.\\n\\nProvided accurate and transparent financial reporting resulting in successful acquittal of MOH grants and a rollover of funds into subsequent years.\\n\\n\\n\\nRoyal Australian & New Zealand College of Radiologist November 2017 – January 2018\\n\\nContract Finance Manager\\n\\nCoordinated activities of 3 financial accountants including training and mentoring. Administered AR, AP, bank reconciliations and general journals. Monitored financial activities and reporting for 60 cost centres ensuring all legal and regulatory requirements met. Directed Australian and New Zealand payroll and superannuation withholdings. Performed tax management including GST and payroll tax. \\n\\nStreamlined external payroll accounting processes.\\n\\nRestructured accounting team functions for enhanced compatibility and efficient operations.\\n\\n\\n\\nKolling Institute of Research June 2017 – September 2017\\n\\nContract Finance Manager\\n\\nContracted for 12 weeks to provide management of annual budget and monthly forecasting. Prepared financial statements, business activity reports, financial position forecasts, annual budgets, and reports required by regulatory agencies. Analysed budgets and financial details of past, present, and expected operations to identify development opportunities and areas of improvement. Conducted year and month end financial management. Collaborated with cost centre managers to monitor and clear deficit cost centres.\\n\\nDetected several overdrawn cost centres through accurate cash flow forecasts and analysis; negotiated clearance and prevention of reoccurrence through MOH permissible fund transfers from alternative cost centres.\\n\\nPrepared allocated cost centre budgets based on three scenario’s adhering to strict deadlines.\\n\\nContributed to risk management review resulting in recommendations for improvement and enhanced internal controls.\\n\\n\\n\\nSchool for Life Ltd February 2017 – April 2017\\n\\nContract Finance Manager\\n\\nMonitored and directed financial activities of the foundation including profit and loss, balance sheet, cash flow and budget analysis. Developed annual budget and coordinated management of GST and FBT taxes.\\n\\nImplemented FBT salary packaging in payroll system.\\n\\nThoroughly cleansed CRM greatly reducing omissions, duplications and recurring donation drop offs.\\n\\nRevamped budget template and introduced detailed budget variance analysis and monthly management reporting to assist in the decision-making process.  \\n\\n\\n\\nCancer Patients Network November 2014 – February 2017\\n\\nFinance Manager\\n\\nPerformed financial, HR and payroll functions. Participated in the external and internal audit process including the preparation of statutory financial reports. Developed and tracked operational, capital, and fund-raising budgets. Supported business cases and provided strategic advice and process improvements. Prepared various financial reports for large investment portfolios and for presentation to executive management Enabled continuous improvement in standards and practices of financial systems through creation of policies and processes. Partnered with Cancer Council of NSW to build strategy for accommodation facility including refurbishment, marketing, sinking fund, tariffs, and outsourcing. Successfully negotiated new contracts and renewals. Administered GST and FBT taxes. Led a team of four direct reports.\\n\\nInstrumental in the delivery of cloud financial software implementation and cloud-based CRM reservation system projects leading to improved timeliness and accuracy of financial and statistical reporting.\\n\\nHarnessed automated bank feeds utilizing cloud system resulting in time and cost savings.\\n\\nInitiated improvements to compliance policy in consultation with the branches documentation policies meeting requirements of external Auditors and avoiding audit qualifications.\\n\\nCentralised 191 sub-entity bank accounts held at disparate banks to Big 4 banks reducing audit costs and delays as well as improving visibility of consolidated cash position.\\n\\nIntroduced sinking fund spreadsheet for organisation’s cancer hostel enabling forecasts of capital expenditure requirements for ten years.\\n\\nProposed use of outsourcing services for cancer hostel saving on housekeeping employee costs.\\n\\n\\n\\n\\n\\nJob Ready Solutions June 2014 – November 2014\\n\\nFinance Manager\\n\\nProcessed annual statutory accounts and arranged BAS, FBT, PAYG and income taxes. Ensured accuracy and integrity of the General Ledger, payroll, receivables, and payables. Liaised with sales, marketing and developers providing support, analysis, and insight. Planned and presented monthly reports to management. Led a team of 2 direct reports.\\n\\nResolved portfolio of bad debts due to errors between CRM and accounting software and other disputes through careful negotiations with debtors. \\n\\nCleared backlog of statutory reporting and cleaned up general ledger prior to due diligence of an equity investor, ensuring successful sale of company.\\n\\n\\n\\nMWP Community Aid, Northern Sydney July 2012 – May 2014\\n\\nFinancial Controller\\n\\nCoordinated the financial planning, accounting, administrative and HR activities of the company. Prepared financial statements, business activity reports, financial position forecasts, annual budgets, and reports required by regulatory agencies. Served as liaison with external auditor. Oversaw cash flow management, forecasting and client cash collections. Analysed the financial details of past, present, and expected operations to identify future revenues and expenses. Provided strategic direction and assistance to management regarding business development, cost control and generation of revenue. Planned, developed, and executed maintenance of financial and administrative systems and policies. Administered and coordinated company insurances, asset registers and fleet vehicles. Led a team of 3 direct reports.\\n\\nSuccessfully guided government audit into acquittal of deferred funding with no loss of funding through careful negotiation with representatives of Federal Department of Health (DOHA) and State Family and Community Services (FACS) resolving issues around deferred funding.\\n\\nEstablished various internal controls, policies, and procedures previously non-existent including Delegation of Authority, Purchase Order System, Authorisation Processes, Procedures for Acquittals, Internal Transaction Processing Protocols and Payroll Policies. \\n\\nRevamped timesheets and leave forms.\\n\\nImproved cash management through the creation of cashflow forecasting spreadsheets. \\n\\nIntroduced monthly cash flow statements, budget variance analysis, individual project variance analysis and financial and risk analysis commentaries for enhanced monthly board reporting.\\n\\nCollaborated with program managers to establish zero-based budgets resulting in improved accuracy of budgeting process.\\n\\nImplemented new MYOB configuration segregating funding for aged and disability and setting up job hierarchies, job tracking and job budgeting leading to improved accuracy of financial reporting and cost control.\\n\\nRecommended solutions for expansion of home modification program to increase profitability\\n\\n\\n\\nGlen Street Theatre, Warringah Council October 2009 – June 2012\\n\\nFinance Manager\\n\\nProvided financial, statistical, and reporting services to executive management. Prepared budgets and quarterly re-forecasts. Liaised with internal and external auditors in preparation of monthly and statutory year-end financial statements. Controlled cash management and prepared cash flow forecasts. Monitored and reported on variance analysis and cost control initiatives. Scrutinized contracts and contractual obligations to ensure compliance. Partnered with Marketing and Technical cost divisions providing technical assistance regarding monitoring budgets, cost control and revenue generation. Tasked with project management accounting for each new show including reconciliations and budget variance analysis. Assisted management team with development of business planning and strategy. Led a team of 3 direct reports.\\n\\nSuccessfully utilized financial models to assess cost benefit analysis of outsourcing aiding decision making process.\\n\\nImproved stock control and reduced stock losses through effective application of physical controls, sharpened record-keeping, and refined configuration of POS software.\\n\\nEnabled cost cutting strategies generating significant ongoing saving through effective utilization of detailed budget variance analysis and monthly management reporting allowing for timely decision making. \\n\\n\\n\\nAdditional Experience\\n\\nSenior Financial Accountant • Reed International Ltd (4 direct reports)\\n\\nFinance Manager • Vivendi Pty Ltd (led a team with 3 direct reports over 7 years)\\n\\n\\n\\nEducation\\n\\nBachelor of Business, Accounting\\n\\nThe University of Technology Sydney, Sydney, 2000\\n\\nAdvanced Diploma in Accounting\\n\\nTAFE, Sydney, 1996\\n\\nLicenses & Certifications\\n\\nCharted Institute of Accountants ANZ, Member, 2017\\n\\nCPA Australia, Member, 2005\\n\\nAwards\\n\\nCPA Corporate Governance \\n\\nCPA Insolvency and Reconstruction\\n\\nReuters Financial Markets Prize: UTS Sydney\\n\\nHighest overall performance in Advanced Diploma of Accounting: Sydney TAFE\\n\\nCorporations Law prize: Sydney TAFE\\n\\nSoftware Skills \\n\\nOracle • Oracle Hyperion • Com-share • Decision-cast • Baan •Essebase • Pastel • Sun Systems • Vision • Technology One • Book-master • MYOB • MYOB Advanced • Xero Certification • ADP Pay Connect Supervisor • FBT Simplifier Software • Attache • Attache for Payroll • Micros Fidelio • RMS Reservations Software\\n\\n • CRM donations and fundraising software: • Thank U • Salesforce • Microsoft Dynamics\\n\\n  Page 1 | 1'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_summary_flag = False\n",
    "exp_summary = ''\n",
    "#for line in contents:\n",
    "#    print(line)\n",
    "while True:\n",
    "    if contents in tuple(work_and_employment): #.startswith(tuple(work_and_employment)):\n",
    "        exp_summary_flag = True\n",
    "        \n",
    "    elif exp_summary_flag:\n",
    "        exp_summary += line\n",
    "        if not line.strip(): break\n",
    "\n",
    "print(exp_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636828dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(work_and_employment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041e1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418336b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
